{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model Training - Simplified Single-Branch Models\n",
    "\n",
    "This notebook trains separate simplified CNN models for each feature type and then creates an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    ")\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from modules.PostgresDBHandler import PostgresDBHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "dbParams = {\n",
    "    \"dbname\": \"mydatabase\",\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"host\": \"postgres_server\",\n",
    "    \"port\": \"5432\",\n",
    "}\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "KFOLD_SPLITS = 5\n",
    "FIXED_LENGTH = 128\n",
    "\n",
    "# Feature types to train models for\n",
    "FEATURE_TYPES = [\n",
    "    'mel_spectrogram', 'mfcc', 'chromagram', 'spectral_contrast',\n",
    "    'tonnetz', 'constant_q', 'cqt', 'stft', 'harmonic_percussive', 'onset_strength'\n",
    "]\n",
    "\n",
    "# Feature shapes for each type\n",
    "FEATURE_SHAPES = {\n",
    "    'mel_spectrogram': (64, 128),\n",
    "    'mfcc': (8, 128),\n",
    "    'chromagram': (8, 128),\n",
    "    'spectral_contrast': (3, 128),\n",
    "    'tonnetz': (6, 128),\n",
    "    'constant_q': (42, 128),\n",
    "    'cqt': (42, 128),\n",
    "    'stft': (512, 128),\n",
    "    'harmonic_percussive': (1025, 128),\n",
    "    'onset_strength': (1, 128)\n",
    "}\n",
    "# GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Number of available GPUs: {len(gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database connection\n",
    "db = PostgresDBHandler(**dbParams)\n",
    "db.connect()\n",
    "\n",
    "# Get instrument mappings\n",
    "instruments_mappings = db.get_mappings_instruments()\n",
    "num_classes = len(instruments_mappings)\n",
    "print(f\"Number of instrument classes: {num_classes}\")\n",
    "print(\"Instruments:\", instruments_mappings['name'].tolist())\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleFeatureDataGenerator(Sequence):\n",
    "    \"\"\"Data generator for single feature type training.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, feature_type, batch_size=32, shuffle=True):\n",
    "        self.df = df\n",
    "        self.feature_type = feature_type\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.df['instrumentID_encoded'] = self.label_encoder.fit_transform(self.df['instrumentID'])\n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "        \n",
    "        self.indices = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        batch_df = self.df.iloc[indices]\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            feature_data = np.load(row['featurePath'])\n",
    "            X.append(feature_data)\n",
    "            y.append(row['instrumentID_encoded'])\n",
    "        \n",
    "        X = np.expand_dims(np.array(X), -1)\n",
    "        y = to_categorical(y, num_classes=self.num_classes)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return self.df['instrumentID_encoded'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model(input_shape, num_classes, model_name=\"simple_cnn\"):    \n",
    "    \"\"\"Create a simplified single-branch CNN model with reduced capacity.\"\"\"\n",
    "    input_layer = Input(shape=(*input_shape, 1), name=f\"{model_name}_input\")\n",
    "    \n",
    "    # Reduced number of filters and layers compared to original\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Only one more conv layer (original had 3)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Reduced dense layers\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Smaller final dense layer\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    output = Dense(num_classes, activation='softmax', name=f\"{model_name}_output\")(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output, name=model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for each feature type\n",
    "db = PostgresDBHandler(**dbParams)\n",
    "db.connect()\n",
    "\n",
    "# Get all processed IDs\n",
    "processed_ids = db.get_all_processed_ids()\n",
    "print(f\"Total processed samples: {len(processed_ids)}\")\n",
    "\n",
    "# Create DataFrames for each feature type\n",
    "df_dict = {}\n",
    "for feature_type in FEATURE_TYPES:\n",
    "    processed_data = db.get_processed_fit_data(processed_ids, feature_type)\n",
    "    \n",
    "    if processed_data:\n",
    "        df = pd.DataFrame(processed_data)\n",
    "        df_dict[feature_type] = df\n",
    "        print(f\"{feature_type}: {len(df)} samples\")\n",
    "    else:\n",
    "        print(f\"Warning: No data found for {feature_type}\")\n",
    "\n",
    "db.close()\n",
    "\n",
    "# Filter to only include feature types with data\n",
    "available_feature_types = list(df_dict.keys())\n",
    "print(f\"\\nAvailable feature types: {available_feature_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training results storage\n",
    "all_results = {}\n",
    "all_models = {}\n",
    "\n",
    "# Train individual models for each feature type\n",
    "for feature_type in tqdm(available_feature_types, desc = \"Feature Types\", Leave = True):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training model for {feature_type}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    df = df_dict[feature_type]\n",
    "    input_shape = FEATURE_SHAPES[feature_type]\n",
    "    \n",
    "    # Initialize results storage for this feature type\n",
    "    feature_results = {\n",
    "        'accuracy_list': [],\n",
    "        'loss_list': [],\n",
    "        'classification_reports': [],\n",
    "        'confusion_matrices': [],\n",
    "        'histories': [],\n",
    "        'models': []\n",
    "    }\n",
    "    \n",
    "    # Cross-validation\n",
    "    kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(list(kf.split(df)), desc = f\"{feature_type} Folds\", Leave = False)):\n",
    "        print(f\"\\n--- Fold {fold + 1}/{KFOLD_SPLITS} ---\")\n",
    "        \n",
    "        # Split data\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        test_df = df.iloc[test_idx].reset_index(drop=True)\n",
    "        \n",
    "        # Further split training data\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            np.arange(len(train_df)), test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        val_df = train_df.iloc[val_indices].reset_index(drop=True)\n",
    "        train_df = train_df.iloc[train_indices].reset_index(drop=True)\n",
    "        \n",
    "        # Create data generators\n",
    "        train_generator = SingleFeatureDataGenerator(train_df, feature_type, batch_size=BATCH_SIZE)\n",
    "        val_generator = SingleFeatureDataGenerator(val_df, feature_type, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_generator = SingleFeatureDataGenerator(test_df, feature_type, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        # Create and compile model\n",
    "        model = create_simple_model(input_shape, num_classes, f\"{feature_type}_model\")\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=15, restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=val_generator,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        feature_results['histories'].append(history.history)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        loss, accuracy = model.evaluate(test_generator, verbose=0)\n",
    "        feature_results['accuracy_list'].append(accuracy)\n",
    "        feature_results['loss_list'].append(loss)\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Predict and generate reports\n",
    "        y_pred = model.predict(test_generator, verbose=0)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_true = test_generator.get_labels()\n",
    "        \n",
    "        # Classification report\n",
    "        report = classification_report(y_true, y_pred_classes, output_dict=True)\n",
    "        feature_results['classification_reports'].append(report)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred_classes).tolist()\n",
    "        feature_results['confusion_matrices'].append(conf_matrix)\n",
    "        \n",
    "        # Save the best model (last one for now)\n",
    "        feature_results['models'].append(model)\n",
    "    \n",
    "    # Store results for this feature type\n",
    "    all_results[feature_type] = feature_results\n",
    "    all_models[feature_type] = feature_results['models'][-1]  # Save the last model\n",
    "    \n",
    "    # Print summary for this feature type\n",
    "    mean_acc = np.mean(feature_results['accuracy_list'])\n",
    "    std_acc = np.std(feature_results['accuracy_list'])\n",
    "    print(f\"\\n{feature_type} - Mean Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble predictions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Creating Ensemble Predictions\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use the last fold of each feature type for ensemble evaluation\n",
    "ensemble_results = {\n",
    "    'accuracy_list': [],\n",
    "    'loss_list': [],\n",
    "    'classification_reports': [],\n",
    "    'confusion_matrices': []\n",
    "}\n",
    "\n",
    "# For simplicity, we'll use the last fold of each feature type\n",
    "for fold in tqdm(range(KFOLD_SPLITS), desc = \"Ensemble Folds\", Leave = True):\n",
    "    print(f\"\\n--- Ensemble Fold {fold + 1}/{KFOLD_SPLITS} ---\")\n",
    "    \n",
    "    # Get predictions from all models for this fold\n",
    "    all_predictions = {}\n",
    "    \n",
    "    for feature_type in available_feature_types:\n",
    "        if feature_type in all_results:\n",
    "            # Get the model from this fold\n",
    "            model = all_results[feature_type]['models'][fold]\n",
    "            \n",
    "            # Get test data for this fold (we need to recreate it)\n",
    "            df = df_dict[feature_type]\n",
    "            kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=42)\n",
    "            train_idx, test_idx = list(kf.split(df))[fold]\n",
    "            test_df = df.iloc[test_idx].reset_index(drop=True)\n",
    "            \n",
    "            test_generator = SingleFeatureDataGenerator(test_df, feature_type, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            \n",
    "            # Get predictions\n",
    "            pred = model.predict(test_generator, verbose=0)\n",
    "            all_predictions[feature_type] = pred\n",
    "            \n",
    "            # Store true labels (should be the same for all feature types)\n",
    "            if 'y_true' not in locals():\n",
    "                y_true = test_generator.get_labels()\n",
    "    \n",
    "    # Simple averaging ensemble\n",
    "    if all_predictions:\n",
    "        ensemble_pred = np.mean(list(all_predictions.values()), axis=0)\n",
    "        ensemble_pred_classes = np.argmax(ensemble_pred, axis=1)\n",
    "        \n",
    "        # Calculate ensemble accuracy\n",
    "        ensemble_accuracy = accuracy_score(y_true, ensemble_pred_classes)\n",
    "        ensemble_results['accuracy_list'].append(ensemble_accuracy)\n",
    "        \n",
    "        print(f\"Ensemble Accuracy: {ensemble_accuracy:.4f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        report = classification_report(y_true, ensemble_pred_classes, output_dict=True)\n",
    "        ensemble_results['classification_reports'].append(report)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_true, ensemble_pred_classes).tolist()\n",
    "        ensemble_results['confusion_matrices'].append(conf_matrix)\n",
    "\n",
    "# Store ensemble results\n",
    "all_results['ensemble'] = ensemble_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and models\n",
    "try:\n",
    "    os.mkdir(\"ensemble_models\")\n",
    "except FileExistsError:\n",
    "    print(\"Folder already exists\")\n",
    "except Exception:\n",
    "    print(\"Unknown error\")\n",
    "\n",
    "# Create version folder\n",
    "date_part = datetime.now().date().__str__().replace('-', '_')\n",
    "last_version = os.listdir(path=\"ensemble_models\") if os.path.exists(\"ensemble_models\") else []\n",
    "last_version = [name.rpartition(\"_v\")[-1] for name in last_version if date_part in name]\n",
    "if len(last_version):\n",
    "    last_version = int(sorted(last_version)[-1])\n",
    "else:\n",
    "    last_version = 0\n",
    "folder_name = f\"{date_part}_v{last_version+1}\"\n",
    "\n",
    "os.makedirs(os.path.join(\"ensemble_models\", folder_name), exist_ok=True)\n",
    "\n",
    "# Save individual models\n",
    "for feature_type, model in all_models.items():\n",
    "    model_path = os.path.join(\"ensemble_models\", folder_name, f\"{feature_type}_model.h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Saved {feature_type} model to {model_path}\")\n",
    "\n",
    "# Save results\n",
    "results_data = {\n",
    "    'individual_results': {ft: {k: v for k, v in res.items() if k != 'models'} \n",
    "                          for ft, res in all_results.items() if ft != 'ensemble'},\n",
    "    'ensemble_results': all_results['ensemble'],\n",
    "    'feature_types': available_feature_types,\n",
    "    'num_classes': num_classes,\n",
    "    'feature_shapes': FEATURE_SHAPES,\n",
    "    'training_config': {\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'kfold_splits': KFOLD_SPLITS,\n",
    "        'fixed_length': FIXED_LENGTH\n",
    "    },\n",
    "    'instrument_mappings': instruments_mappings.to_dict()\n",
    "}\n",
    "\n",
    "results_path = os.path.join(\"ensemble_models\", folder_name, \"results.json\")\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "print(f\"Models saved to: ensemble_models/{folder_name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nIndividual Model Performance:\")\n",
    "for feature_type in available_feature_types:\n",
    "    if feature_type in all_results:\n",
    "        accuracies = all_results[feature_type]['accuracy_list']\n",
    "        mean_acc = np.mean(accuracies)\n",
    "        std_acc = np.std(accuracies)\n",
    "        print(f\"  {feature_type}: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "\n",
    "print(\"\\nEnsemble Performance:\")\n",
    "if 'ensemble' in all_results:\n",
    "    ensemble_accuracies = all_results['ensemble']['accuracy_list']\n",
    "    ensemble_mean = np.mean(ensemble_accuracies)\n",
    "    ensemble_std = np.std(ensemble_accuracies)\n",
    "    print(f\"  Ensemble: {ensemble_mean:.4f} ± {ensemble_std:.4f}\")\n",
    "\n",
    "# Find best individual model\n",
    "best_individual = max(\n",
    "    [(ft, np.mean(all_results[ft]['accuracy_list'])) \n",
    "     for ft in available_feature_types if ft in all_results],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "improvement = ensemble_mean - best_individual[1]\n",
    "print(f\"\\nBest Individual Model: {best_individual[0]} ({best_individual[1]:.4f})\")\n",
    "print(f\"Ensemble Improvement: {improvement:.4f} ({improvement*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
