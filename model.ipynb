{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model Training - Simplified Single-Branch Models\n",
    "\n",
    "This notebook trains separate simplified CNN models for each feature type and then creates an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    ")\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from modules.PostgresDBHandler import PostgresDBHandler\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "dbParams = {\n",
    "    \"dbname\": \"mydatabase\",\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"host\": \"postgres_server\",\n",
    "    \"port\": \"5432\",\n",
    "}\n",
    "\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "KFOLD_SPLITS = 5\n",
    "FIXED_LENGTH = 128\n",
    "\n",
    "# Feature types to train models for\n",
    "FEATURE_TYPES = [\n",
    "    'mel_spectrogram', 'mfcc', 'chromagram', 'spectral_contrast',\n",
    "    'tonnetz', 'constant_q', 'cqt', 'stft', 'harmonic_percussive', 'onset_strength'\n",
    "]\n",
    "\n",
    "# GPU configuration\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Number of available GPUs: {len(gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database connection\n",
    "db = PostgresDBHandler(**dbParams)\n",
    "db.connect()\n",
    "\n",
    "# Get instrument mappings\n",
    "instruments_mappings = db.get_mappings_instruments()\n",
    "num_classes = len(instruments_mappings)\n",
    "print(f\"Number of instrument classes: {num_classes}\")\n",
    "print(\"Instruments:\", instruments_mappings['name'].tolist())\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbConnect = PostgresDBHandler(**dbParams)\n",
    "dbConnect.connect()\n",
    "audioIDs = dbConnect.get_all_unique_audio_ids_in_processed()\n",
    "processed_data = dbConnect.get_processed_fit_data(audioIDs)\n",
    "\n",
    "all_processed_data = []\n",
    "for audio_id in audioIDs:\n",
    "    features = dbConnect.get_all_feature_types_for_audio(audio_id)\n",
    "    feature_dict = {f['featureTypeName']: f['featurePath'] for f in features}\n",
    "    instrumentID = dbConnect.get_audio_file(audio_id)['instrumentID']\n",
    "    feature_dict['instrumentID'] = instrumentID\n",
    "    all_processed_data.append(feature_dict)\n",
    "\n",
    "dbConnect.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = pd.DataFrame(all_processed_data)\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_shape(feature_type, df):\n",
    "    feature_path_col = feature_type \n",
    "    for path in df[feature_path_col]:\n",
    "        if isinstance(path, str) and os.path.exists(path):\n",
    "            arr = np.load(path)\n",
    "            return arr.shape\n",
    "    raise ValueError(f\"No valid file found for {feature_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleFeatureDataGenerator(Sequence):\n",
    "    def __init__(self, df, feature_col, batch_size=32, shuffle=True, num_classes=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.feature_col = feature_col\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_classes = num_classes\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        for _, row in batch_df.iterrows():\n",
    "            try:\n",
    "                arr = np.load(row[self.feature_col])\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {row[self.feature_col]}: {e}\")\n",
    "                continue\n",
    "        \n",
    "            if np.isnan(arr).any() or np.isinf(arr).any():\n",
    "                raise ValueError(f\"Feature file {row[self.feature_col]} contains NaNs or Infs.\")\n",
    "        \n",
    "            arr = (arr - np.mean(arr)) / (np.std(arr) + 1e-8)\n",
    "            if arr.ndim == 2:\n",
    "                arr = np.expand_dims(arr, -1)  # shape: (H, W, 1)\n",
    "        \n",
    "            X.append(arr)\n",
    "            y.append(row['instrumentID'])  # already label-encoded\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = to_categorical(np.array(y), num_classes=self.num_classes)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model(input_shape, num_classes, model_name=\"simple_cnn\"):\n",
    "    input_layer = Input(shape=(*input_shape, 1), name=f\"{model_name}_input\")\n",
    "\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    output = Dense(num_classes, activation='softmax', name=f\"{model_name}_output\")(x)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output, name=model_name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for feature_type in tqdm(FEATURE_TYPES, desc=\"Training features\"):\n",
    "    print(f\"\\n{'='*40}\\nTraining model for {feature_type}\\n{'='*40}\")\n",
    "\n",
    "    feature_col = feature_type\n",
    "    feature_df = processed_df.dropna(subset=[feature_col])\n",
    "    \n",
    "    # Global label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(feature_df['instrumentID'])\n",
    "\n",
    "    feature_df = feature_df.copy()\n",
    "    feature_df['instrumentID'] = label_encoder.transform(feature_df['instrumentID'])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    input_shape = get_input_shape(feature_type, feature_df)\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=42)\n",
    "    accuracy_list, loss_list, history_list = [], [], []\n",
    "    classification_reports, confusion_matrices = [], []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(feature_df)):\n",
    "        print(f\"\\n--- Fold {fold+1}/{KFOLD_SPLITS} ---\")\n",
    "        train_df = feature_df.iloc[train_idx].reset_index(drop=True)\n",
    "        test_df = feature_df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "        train_df, val_df = train_test_split(\n",
    "            train_df, test_size=0.2, random_state=42, stratify=train_df['instrumentID'])\n",
    "\n",
    "        # Generators (labels are already encoded)\n",
    "        train_gen = SingleFeatureDataGenerator(train_df, feature_col, BATCH_SIZE, shuffle=True, num_classes=num_classes)\n",
    "        val_gen   = SingleFeatureDataGenerator(val_df,   feature_col, BATCH_SIZE, shuffle=False, num_classes=num_classes)\n",
    "        test_gen  = SingleFeatureDataGenerator(test_df,  feature_col, BATCH_SIZE, shuffle=False, num_classes=num_classes)\n",
    "\n",
    "        # Model\n",
    "        model = create_simple_model(input_shape, num_classes, model_name=feature_type)\n",
    "        model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, callbacks=[early_stopping])\n",
    "        history_list.append(history.history)\n",
    "\n",
    "        # Evaluation\n",
    "        loss, acc = model.evaluate(test_gen)\n",
    "        loss_list.append(loss)\n",
    "        accuracy_list.append(acc)\n",
    "        print(f\"{feature_type} - Fold {fold+1} Test accuracy: {acc:.4f}\")\n",
    "\n",
    "        # Predictions & Reports\n",
    "        y_pred = model.predict(test_gen)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        y_true = []\n",
    "        for _, labels in test_gen:\n",
    "            y_true.extend(np.argmax(labels, axis=1))\n",
    "        y_true = np.array(y_true)\n",
    "\n",
    "        report = classification_report(y_true, y_pred_classes, output_dict=True)\n",
    "        classification_reports.append(report)\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred_classes).tolist()\n",
    "        confusion_matrices.append(conf_matrix)\n",
    "\n",
    "        # Save model\n",
    "        os.makedirs(f\"models/{feature_type}\", exist_ok=True)\n",
    "        model.save(f\"models/{feature_type}/model_fold{fold+1}.keras\")\n",
    "\n",
    "    # Save results\n",
    "    results[feature_type] = {\n",
    "        \"accuracy_list\": accuracy_list,\n",
    "        \"loss_list\": loss_list,\n",
    "        \"histories\": history_list,\n",
    "        \"classification_reports\": classification_reports,\n",
    "        \"confusion_matrices\": confusion_matrices,\n",
    "    }\n",
    "\n",
    "    with open(f\"models/{feature_type}/results.json\", \"w\") as f:\n",
    "        json.dump(results[feature_type], f, indent=2)\n",
    "\n",
    "print(\"\\nAll training complete. Models and results saved in 'models/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Creating Ensemble Predictions\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "ensemble_accuracies = []\n",
    "ensemble_reports = []\n",
    "ensemble_conf_matrices = []\n",
    "\n",
    "for fold in range(KFOLD_SPLITS):\n",
    "    print(f\"\\n--- Ensemble Fold {fold + 1}/{KFOLD_SPLITS} ---\")\n",
    "    fold_preds = []\n",
    "    y_true = None\n",
    "\n",
    "    for feature_type in FEATURE_TYPES:\n",
    "        # Load model for this fold\n",
    "        model_path = f\"models/{feature_type}/model_fold{fold+1}.keras\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Model not found: {model_path}\")\n",
    "            continue\n",
    "        model = keras.models.load_model(model_path)\n",
    "\n",
    "        # Get test data for this fold\n",
    "        feature_df = processed_df[[feature_type, 'instrumentID']].dropna().reset_index(drop=True)\n",
    "        kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=42)\n",
    "        train_idx, test_idx = list(kf.split(feature_df))[fold]\n",
    "        test_df = feature_df.iloc[test_idx].reset_index(drop=True)\n",
    "        test_gen = SingleFeatureDataGenerator(test_df, feature_type, batch_size=BATCH_SIZE, shuffle=False, num_classes=num_classes)\n",
    "        preds = model.predict(test_gen, verbose=0)\n",
    "        fold_preds.append(preds)\n",
    "        if y_true is None:\n",
    "            # Get true labels from generator\n",
    "            y_true = []\n",
    "            for _, labels in test_gen:\n",
    "                y_true.extend(np.argmax(labels, axis=1))\n",
    "            y_true = np.array(y_true)\n",
    "\n",
    "    if fold_preds:\n",
    "        ensemble_pred = np.mean(fold_preds, axis=0)\n",
    "        ensemble_pred_classes = np.argmax(ensemble_pred, axis=1)\n",
    "        acc = accuracy_score(y_true, ensemble_pred_classes)\n",
    "        ensemble_accuracies.append(acc)\n",
    "        print(f\"Ensemble Accuracy: {acc:.4f}\")\n",
    "        report = classification_report(y_true, ensemble_pred_classes, output_dict=True)\n",
    "        ensemble_reports.append(report)\n",
    "        conf_matrix = confusion_matrix(y_true, ensemble_pred_classes).tolist()\n",
    "        ensemble_conf_matrices.append(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"ensemble_results\", exist_ok=True)\n",
    "date_part = datetime.now().date().__str__().replace('-', '_')\n",
    "results_path = os.path.join(\"ensemble_results\", f\"ensemble_results_{date_part}.json\")\n",
    "ensemble_results = {\n",
    "    \"accuracy_list\": ensemble_accuracies,\n",
    "    \"classification_reports\": ensemble_reports,\n",
    "    \"confusion_matrices\": ensemble_conf_matrices,\n",
    "}\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(ensemble_results, f, indent=2)\n",
    "print(f\"\\nEnsemble results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nIndividual Model Performance:\")\n",
    "for feature_type in FEATURE_TYPES:\n",
    "    if feature_type in results:\n",
    "        accuracies = results[feature_type]['accuracy_list']\n",
    "        mean_acc = np.mean(accuracies)\n",
    "        std_acc = np.std(accuracies)\n",
    "        print(f\"  {feature_type}: {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "\n",
    "print(\"\\nEnsemble Performance:\")\n",
    "ensemble_mean = np.mean(ensemble_accuracies)\n",
    "ensemble_std = np.std(ensemble_accuracies)\n",
    "print(f\"  Ensemble: {ensemble_mean:.4f} ± {ensemble_std:.4f}\")\n",
    "\n",
    "# Find best individual model\n",
    "best_individual = max(\n",
    "    [(ft, np.mean(results[ft]['accuracy_list'])) for ft in FEATURE_TYPES if ft in results],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "improvement = ensemble_mean - best_individual[1]\n",
    "print(f\"\\nBest Individual Model: {best_individual[0]} ({best_individual[1]:.4f})\")\n",
    "print(f\"Ensemble Improvement: {improvement:.4f} ({improvement*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
