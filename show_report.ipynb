{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Report Display - Individual Models vs Ensemble\n",
    "\n",
    "This notebook displays comprehensive results comparing individual feature-based models with ensemble performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest ensemble results file\n",
    "ensemble_results_files = sorted(glob.glob(\"ensemble_results/ensemble_results_*.json\"), reverse=True)\n",
    "if not ensemble_results_files:\n",
    "    raise FileNotFoundError(\"No ensemble results file found in ensemble_results/ directory.\")\n",
    "ENSEMBLE_RESULTS_PATH = ensemble_results_files[0]\n",
    "ENSEMBLE_RESULTS_DIR = os.path.dirname(ENSEMBLE_RESULTS_PATH)\n",
    "\n",
    "with open(ENSEMBLE_RESULTS_PATH, \"r\") as file:\n",
    "    ensemble_results = json.load(file)\n",
    "\n",
    "print(f\"Loaded ensemble results from: {ENSEMBLE_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy(accuracy_list, title=\"Accuracy\"):\n",
    "    \"\"\"Display accuracy statistics.\"\"\"\n",
    "    print(f\"{title} List:\", accuracy_list)\n",
    "    print(f\"{title} mean: {np.array(accuracy_list).mean():.4f}\")\n",
    "    print(f\"{title} std: {np.array(accuracy_list).std():.4f}\")\n",
    "    print(f\"{title} min: {np.array(accuracy_list).min():.4f}\")\n",
    "    print(f\"{title} max: {np.array(accuracy_list).max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_classification_reports(classification_reports, mappings, title=\"Classification Report\"):\n",
    "    \"\"\"Display classification reports.\"\"\"\n",
    "    for i, report in enumerate(classification_reports):\n",
    "        dfReport = pd.DataFrame(report).transpose()\n",
    "        dfReport.rename(index=mappings, inplace=True)\n",
    "        print(f\"\\n{title} {i+1}:\")\n",
    "        print(dfReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title=\"Training History\"):\n",
    "    \"\"\"Plot training history.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history[\"accuracy\"])\n",
    "    plt.plot(history[\"val_accuracy\"])\n",
    "    plt.title(f\"{title} - Accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history[\"loss\"])\n",
    "    plt.plot(history[\"val_loss\"])\n",
    "    plt.title(f\"{title} - Loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(confusion_matrices, mappings, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrices.\"\"\"\n",
    "    for i, conf_matrix in enumerate(confusion_matrices):\n",
    "        conf_matrix = np.array(conf_matrix)\n",
    "        if conf_matrix.ndim == 1:\n",
    "            conf_matrix = conf_matrix.reshape(-1, 1)\n",
    "        \n",
    "        labels = [mappings[str(i)] for i in range(len(mappings))]\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "        plt.yticks(rotation=45)\n",
    "        plt.title(f\"{title} - Cross validation {i+1}\")\n",
    "        plt.xlabel(\"Predicted Labels\")\n",
    "        plt.ylabel(\"True Labels\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Model Performance Analysis\n",
    "print(\"=\"*60)\n",
    "print(\"INDIVIDUAL MODEL PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "individual_results = ensemble_data['individual_results']\n",
    "instrument_mappings = ensemble_data['instrument_mappings']['name']\n",
    "\n",
    "# Create performance summary DataFrame\n",
    "performance_data = []\n",
    "for feature_type, results in individual_results.items():\n",
    "    accuracies = results['accuracy_list']\n",
    "    losses = results['loss_list']\n",
    "    \n",
    "    performance_data.append({\n",
    "        'Feature Type': feature_type.replace('_', ' ').title(),\n",
    "        'Mean Accuracy': np.mean(accuracies),\n",
    "        'Std Accuracy': np.std(accuracies),\n",
    "        'Min Accuracy': np.min(accuracies),\n",
    "        'Max Accuracy': np.max(accuracies),\n",
    "        'Mean Loss': np.mean(losses),\n",
    "        'Std Loss': np.std(losses)\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values('Mean Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nIndividual Model Performance Summary:\")\n",
    "print(performance_df.round(4))\n",
    "\n",
    "# Show detailed results for each feature type\n",
    "for feature_type, results in individual_results.items():\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"{feature_type.replace('_', ' ').title()} Model\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    show_accuracy(results['accuracy_list'], f\"{feature_type} Accuracy\")\n",
    "    show_accuracy(results['loss_list'], f\"{feature_type} Loss\")\n",
    "    \n",
    "    # Display classification reports\n",
    "    display_classification_reports(\n",
    "        results['classification_reports'], \n",
    "        instrument_mappings, \n",
    "        f\"{feature_type} Classification Report\"\n",
    "    )\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    plot_confusion(\n",
    "        results['confusion_matrices'], \n",
    "        instrument_mappings, \n",
    "        f\"{feature_type} Confusion Matrix\"\n",
    "    )\n",
    "    \n",
    "    # Plot training histories (first fold only for brevity)\n",
    "    if results['histories']:\n",
    "        plot_history(\n",
    "            results['histories'][0], \n",
    "            f\"{feature_type} Training History (Fold 1)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Performance Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ensemble_results = ensemble_data['ensemble_results']\n",
    "\n",
    "print(\"\\nEnsemble Model Performance:\")\n",
    "show_accuracy(ensemble_results['accuracy_list'], \"Ensemble Accuracy\")\n",
    "\n",
    "# Display ensemble classification reports\n",
    "display_classification_reports(\n",
    "    ensemble_results['classification_reports'], \n",
    "    instrument_mappings, \n",
    "    \"Ensemble Classification Report\"\n",
    ")\n",
    "\n",
    "# Plot ensemble confusion matrices\n",
    "plot_confusion(\n",
    "    ensemble_results['confusion_matrices'], \n",
    "    instrument_mappings, \n",
    "    \"Ensemble Confusion Matrix\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Visualizations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Performance Comparison Bar Chart\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "feature_names = [ft.replace('_', '\\n') for ft in performance_df['Feature Type']]\n",
    "mean_accuracies = performance_df['Mean Accuracy'].values\n",
    "std_accuracies = performance_df['Std Accuracy'].values\n",
    "\n",
    "# Individual models\n",
    "bars1 = plt.bar(range(len(feature_names)), mean_accuracies, \n",
    "                yerr=std_accuracies, capsize=5, alpha=0.8, \n",
    "                label='Individual Models')\n",
    "\n",
    "# Ensemble (horizontal line)\n",
    "ensemble_mean = np.mean(ensemble_results['accuracy_list'])\n",
    "ensemble_std = np.std(ensemble_results['accuracy_list'])\n",
    "plt.axhline(y=ensemble_mean, color='red', linestyle='--', linewidth=3, \n",
    "            label=f'Ensemble ({ensemble_mean:.4f})')\n",
    "plt.fill_between([-0.5, len(feature_names)-0.5], \n",
    "                 ensemble_mean - ensemble_std, ensemble_mean + ensemble_std, \n",
    "                 alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel('Feature Types')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Individual Models vs Ensemble Performance')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Box Plot of Accuracy Distributions\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "data_for_box = [individual_results[ft]['accuracy_list'] \n",
    "                for ft in performance_df['Feature Type'].str.replace(' ', '_').str.lower()]\n",
    "labels = [ft.replace('_', '\\n') for ft in performance_df['Feature Type']]\n",
    "\n",
    "box_plot = plt.boxplot(data_for_box, labels=labels)\n",
    "plt.axhline(y=ensemble_mean, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Ensemble Mean ({ensemble_mean:.4f})')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Distribution Across Folds')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Improvement Analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "improvements = []\n",
    "for ft in performance_df['Feature Type'].str.replace(' ', '_').str.lower():\n",
    "    individual_mean = np.mean(individual_results[ft]['accuracy_list'])\n",
    "    improvement = ensemble_mean - individual_mean\n",
    "    improvements.append(improvement)\n",
    "\n",
    "colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "bars = plt.bar(range(len(feature_names)), improvements, color=colors, alpha=0.7)\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "plt.xlabel('Feature Types')\n",
    "plt.ylabel('Improvement (Ensemble - Individual)')\n",
    "plt.title('Ensemble Improvement Over Individual Models')\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, imp) in enumerate(zip(bars, improvements)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "            f'{imp:.4f}', ha='center', va='bottom' if imp > 0 else 'top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find best and worst individual models\n",
    "best_individual = performance_df.iloc[0]\n",
    "worst_individual = performance_df.iloc[-1]\n",
    "\n",
    "print(f\"\\nBest Individual Model: {best_individual['Feature Type']}\")\n",
    "print(f\"  Accuracy: {best_individual['Mean Accuracy']:.4f} ± {best_individual['Std Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nWorst Individual Model: {worst_individual['Feature Type']}\")\n",
    "print(f\"  Accuracy: {worst_individual['Mean Accuracy']:.4f} ± {worst_individual['Std Accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nEnsemble Performance:\")\n",
    "print(f\"  Accuracy: {ensemble_mean:.4f} ± {ensemble_std:.4f}\")\n",
    "\n",
    "# Calculate improvements\n",
    "improvement_over_best = ensemble_mean - best_individual['Mean Accuracy']\n",
    "improvement_over_worst = ensemble_mean - worst_individual['Mean Accuracy']\n",
    "\n",
    "print(f\"\\nImprovement over best individual: {improvement_over_best:.4f} ({improvement_over_best*100:.2f}%)\")\n",
    "print(f\"Improvement over worst individual: {improvement_over_worst:.4f} ({improvement_over_worst*100:.2f}%)\")\n",
    "\n",
    "# Model consistency analysis\n",
    "print(f\"\\nModel Consistency (lower std = more consistent):\")\n",
    "consistency_ranking = performance_df.sort_values('Std Accuracy')\n",
    "for _, row in consistency_ranking.iterrows():\n",
    "    print(f\"  {row['Feature Type']}: {row['Std Accuracy']:.4f}\")\n",
    "\n",
    "# Feature type analysis by category\n",
    "print(f\"\\nFeature Type Analysis by Category:\")\n",
    "spectral_features = ['mel_spectrogram', 'stft', 'constant_q', 'harmonic_percussive']\n",
    "cepstral_features = ['mfcc']\n",
    "harmonic_features = ['chromagram', 'cqt', 'tonnetz']\n",
    "other_features = ['spectral_contrast', 'onset_strength']\n",
    "\n",
    "feature_categories = {\n",
    "    'Spectral': spectral_features,\n",
    "    'Cepstral': cepstral_features,\n",
    "    'Harmonic': harmonic_features,\n",
    "    'Other': other_features\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    category_accuracies = []\n",
    "    for feature in features:\n",
    "        if feature in individual_results:\n",
    "            category_accuracies.extend(individual_results[feature]['accuracy_list'])\n",
    "    \n",
    "    if category_accuracies:\n",
    "        category_mean = np.mean(category_accuracies)\n",
    "        category_std = np.std(category_accuracies)\n",
    "        print(f\"  {category}: {category_mean:.4f} ± {category_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive report\n",
    "def save_comprehensive_report():\n",
    "    \"\"\"Save a comprehensive analysis report.\"\"\"\n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'ensemble_path': ENSEMBLE_REPORT_PATH,\n",
    "        'summary': {\n",
    "            'num_instruments': len(instrument_mappings),\n",
    "            'num_feature_types': len(individual_results),\n",
    "            'num_folds': len(ensemble_results['accuracy_list']),\n",
    "            'training_config': ensemble_data['training_config']\n",
    "        },\n",
    "        'performance': {\n",
    "            'individual_models': performance_df.to_dict('records'),\n",
    "            'ensemble': {\n",
    "                'mean': ensemble_mean,\n",
    "                'std': ensemble_std,\n",
    "                'min': np.min(ensemble_results['accuracy_list']),\n",
    "                'max': np.max(ensemble_results['accuracy_list'])\n",
    "            }\n",
    "        },\n",
    "        'comparison': {\n",
    "            'best_individual': best_individual['Feature Type'],\n",
    "            'best_individual_accuracy': best_individual['Mean Accuracy'],\n",
    "            'improvement_over_best': improvement_over_best,\n",
    "            'improvement_over_worst': improvement_over_worst\n",
    "        },\n",
    "        'instrument_classes': list(instrument_mappings.values())\n",
    "    }\n",
    "    \n",
    "    # Save report\n",
    "    report_path = os.path.join(ENSEMBLE_REPORT_PATH, 'comprehensive_analysis.json')\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nComprehensive analysis report saved to: {report_path}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save report\n",
    "comprehensive_report = save_comprehensive_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
