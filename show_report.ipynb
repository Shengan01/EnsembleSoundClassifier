{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Report Display - Individual Models vs Ensemble\n",
    "\n",
    "This notebook displays comprehensive results comparing individual feature-based models with ensemble performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Feature types used in your project\n",
    "FEATURE_TYPES = [\n",
    "    'mel_spectrogram', 'mfcc', 'chromagram', 'spectral_contrast',\n",
    "    'tonnetz', 'constant_q', 'cqt', 'stft', 'harmonic_percussive', 'onset_strength'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and load the latest ensemble results\n",
    "ensemble_results_files = sorted(glob.glob(\"ensemble_results/ensemble_results_*.json\"), reverse=True)\n",
    "if not ensemble_results_files:\n",
    "    raise FileNotFoundError(\"No ensemble results file found in ensemble_results/ directory.\")\n",
    "ENSEMBLE_RESULTS_PATH = ensemble_results_files[0]\n",
    "\n",
    "with open(ENSEMBLE_RESULTS_PATH, \"r\") as file:\n",
    "    ensemble_results = json.load(file)\n",
    "\n",
    "print(f\"Loaded ensemble results from: {ENSEMBLE_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load individual model results for all feature types\n",
    "individual_results = {}\n",
    "for feature_type in FEATURE_TYPES:\n",
    "    results_path = f\"models/{feature_type}/results.json\"\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, \"r\") as f:\n",
    "            individual_results[feature_type] = json.load(f)\n",
    "    else:\n",
    "        print(f\"Results not found for {feature_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ensemble accuracy statistics\n",
    "accs = ensemble_results['accuracy_list']\n",
    "print(\"Ensemble Accuracies (per fold):\", accs)\n",
    "print(f\"Mean Ensemble Accuracy: {np.mean(accs):.4f}\")\n",
    "print(f\"Std Ensemble Accuracy: {np.std(accs):.4f}\")\n",
    "print(f\"Min Ensemble Accuracy: {np.min(accs):.4f}\")\n",
    "print(f\"Max Ensemble Accuracy: {np.max(accs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ensemble classification reports for each fold\n",
    "for i, report in enumerate(ensemble_results['classification_reports']):\n",
    "    print(f\"\\nEnsemble Classification Report for Fold {i+1}:\")\n",
    "    display(pd.DataFrame(report).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display ensemble confusion matrices for each fold\n",
    "for i, conf_matrix in enumerate(ensemble_results['confusion_matrices']):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(np.array(conf_matrix), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Ensemble Confusion Matrix - Fold {i+1}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame for individual models\n",
    "performance_data = []\n",
    "for feature_type, results in individual_results.items():\n",
    "    accuracies = results['accuracy_list']\n",
    "    losses = results['loss_list']\n",
    "    performance_data.append({\n",
    "        'Feature Type': feature_type.replace('_', ' ').title(),\n",
    "        'Mean Accuracy': np.mean(accuracies),\n",
    "        'Std Accuracy': np.std(accuracies),\n",
    "        'Min Accuracy': np.min(accuracies),\n",
    "        'Max Accuracy': np.max(accuracies),\n",
    "        'Mean Loss': np.mean(losses),\n",
    "        'Std Loss': np.std(losses)\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values('Mean Accuracy', ascending=False)\n",
    "display(performance_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed results for each feature type\n",
    "for feature_type, results in individual_results.items():\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"{feature_type.replace('_', ' ').title()} Model\")\n",
    "    print(f\"{'='*40}\")\n",
    "    accs = results['accuracy_list']\n",
    "    print(f\"Accuracies: {accs}\")\n",
    "    print(f\"Mean: {np.mean(accs):.4f}, Std: {np.std(accs):.4f}, Min: {np.min(accs):.4f}, Max: {np.max(accs):.4f}\")\n",
    "\n",
    "    # Classification reports\n",
    "    for i, report in enumerate(results['classification_reports']):\n",
    "        print(f\"\\nClassification Report for Fold {i+1}:\")\n",
    "        display(pd.DataFrame(report).transpose())\n",
    "\n",
    "    # Confusion matrices\n",
    "    for i, conf_matrix in enumerate(results['confusion_matrices']):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(np.array(conf_matrix), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(f\"{feature_type.replace('_', ' ').title()} Confusion Matrix - Fold {i+1}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of individual vs ensemble performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(performance_df['Feature Type'], performance_df['Mean Accuracy'], yerr=performance_df['Std Accuracy'], capsize=5, label='Individual Models')\n",
    "plt.axhline(np.mean(ensemble_results['accuracy_list']), color='red', linestyle='--', label='Ensemble Mean')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Individual Models vs Ensemble Performance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/validation accuracy and loss for each model and fold\n",
    "for feature_type, results in individual_results.items():\n",
    "    histories = results.get('histories', [])\n",
    "    if not histories:\n",
    "        continue\n",
    "    print(f\"\\n{'='*40}\\n{feature_type.replace('_', ' ').title()} Training History\\n{'='*40}\")\n",
    "    for fold, history in enumerate(histories):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        # Accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.get(\"accuracy\", []), label=\"Train\")\n",
    "        plt.plot(history.get(\"val_accuracy\", []), label=\"Validation\")\n",
    "        plt.title(f\"{feature_type.replace('_', ' ').title()} - Fold {fold+1} Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        # Loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.get(\"loss\", []), label=\"Train\")\n",
    "        plt.plot(history.get(\"val_loss\", []), label=\"Validation\")\n",
    "        plt.title(f\"{feature_type.replace('_', ' ').title()} - Fold {fold+1} Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best individual model by mean accuracy\n",
    "best_model = performance_df.iloc[0]['Feature Type'].replace(' ', '_').lower()\n",
    "ensemble_acc = np.array(ensemble_results['accuracy_list'])\n",
    "individual_acc = np.array(individual_results[best_model]['accuracy_list'])\n",
    "\n",
    "t_stat, p_val = ttest_rel(ensemble_acc, individual_acc)\n",
    "print(f\"Paired t-test p-value (ensemble vs best individual): {p_val:.4e}\")\n",
    "if p_val < 0.05:\n",
    "    print(\"Difference is statistically significant (p < 0.05)\")\n",
    "else:\n",
    "    print(\"Difference is NOT statistically significant (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare per-class F1, Precision, Recall between ensemble and best individual model\n",
    "best_model = performance_df.iloc[0]['Feature Type'].replace(' ', '_').lower()\n",
    "ensemble_f1, individual_f1 = [], []\n",
    "ensemble_prec, individual_prec = [], []\n",
    "ensemble_rec, individual_rec = [], []\n",
    "class_labels = None\n",
    "\n",
    "for fold in range(len(ensemble_results['classification_reports'])):\n",
    "    e_report = ensemble_results['classification_reports'][fold]\n",
    "    i_report = individual_results[best_model]['classification_reports'][fold]\n",
    "    if class_labels is None:\n",
    "        class_labels = [k for k in e_report.keys() if k.isdigit()]\n",
    "    ensemble_f1.append([e_report[c]['f1-score'] for c in class_labels])\n",
    "    individual_f1.append([i_report[c]['f1-score'] for c in class_labels])\n",
    "    ensemble_prec.append([e_report[c]['precision'] for c in class_labels])\n",
    "    individual_prec.append([i_report[c]['precision'] for c in class_labels])\n",
    "    ensemble_rec.append([e_report[c]['recall'] for c in class_labels])\n",
    "    individual_rec.append([i_report[c]['recall'] for c in class_labels])\n",
    "\n",
    "ensemble_f1 = np.array(ensemble_f1)\n",
    "individual_f1 = np.array(individual_f1)\n",
    "ensemble_prec = np.array(ensemble_prec)\n",
    "individual_prec = np.array(individual_prec)\n",
    "ensemble_rec = np.array(ensemble_rec)\n",
    "individual_rec = np.array(individual_rec)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot([ensemble_f1[:,i] for i in range(len(class_labels))], positions=np.arange(len(class_labels))-0.2, widths=0.3, patch_artist=True, boxprops=dict(facecolor='red', alpha=0.3), labels=class_labels)\n",
    "plt.boxplot([individual_f1[:,i] for i in range(len(class_labels))], positions=np.arange(len(class_labels))+0.2, widths=0.3, patch_artist=True, boxprops=dict(facecolor='blue', alpha=0.3))\n",
    "plt.xticks(np.arange(len(class_labels)), class_labels)\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Per-Class F1: Ensemble (red) vs Best Individual (blue)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot([ensemble_prec[:,i] for i in range(len(class_labels))], positions=np.arange(len(class_labels))-0.2, widths=0.3, patch_artist=True, boxprops=dict(facecolor='red', alpha=0.3), labels=class_labels)\n",
    "plt.boxplot([individual_prec[:,i] for i in range(len(class_labels))], positions=np.arange(len(class_labels))+0.2, widths=0.3, patch_artist=True, boxprops=dict(facecolor='blue', alpha=0.3))\n",
    "plt.xticks(np.arange(len(class_labels)), class_labels)\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Per-Class Precision: Ensemble (red) vs Best Individual (blue)')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot([ensemble_rec[:,i] for i in range(len(class_labels))], positions=np.arange(len(class_labels))-0.2, widths=0.3, patch_artist=True, boxprops=dict(facecolor='red', alpha=0.3), labels=class_labels)\n",
    "plt.boxplot([individual_rec[:,i] for i in range(len(class_labels))], positions=np.arange(len(class_labels))+0.2, widths=0.3, patch_artist=True, boxprops=dict(facecolor='blue', alpha=0.3))\n",
    "plt.xticks(np.arange(len(class_labels)), class_labels)\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Per-Class Recall: Ensemble (red) vs Best Individual (blue)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top-3 most confused class pairs for ensemble and best individual (by sum of off-diagonal confusion)\n",
    "def top_confusions(conf_matrices, class_labels, top_n=3):\n",
    "    total_conf = np.sum(np.array(conf_matrices), axis=0)\n",
    "    np.fill_diagonal(total_conf, 0)\n",
    "    flat = total_conf.flatten()\n",
    "    top_idx = flat.argsort()[-top_n:][::-1]\n",
    "    for idx in top_idx:\n",
    "        i, j = divmod(idx, total_conf.shape[1])\n",
    "        print(f\"True: {class_labels[i]}, Pred: {class_labels[j]}, Count: {total_conf[i, j]}\")\n",
    "\n",
    "print(\"Top 3 most confused class pairs (Ensemble):\")\n",
    "top_confusions(ensemble_results['confusion_matrices'], class_labels)\n",
    "\n",
    "print(\"\\nTop 3 most confused class pairs (Best Individual):\")\n",
    "top_confusions(individual_results[best_model]['confusion_matrices'], class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of (ensemble accuracy - individual accuracy) per feature type\n",
    "improvements = []\n",
    "for ft in FEATURE_TYPES:\n",
    "    if ft in individual_results:\n",
    "        individual_mean = np.mean(individual_results[ft]['accuracy_list'])\n",
    "        improvement = np.mean(ensemble_results['accuracy_list']) - individual_mean\n",
    "        improvements.append(improvement)\n",
    "    else:\n",
    "        improvements.append(np.nan)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar([ft.replace('_', ' ').title() for ft in FEATURE_TYPES], improvements, color=['green' if imp > 0 else 'red' for imp in improvements])\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.ylabel('Ensemble Improvement over Individual')\n",
    "plt.title('Ensemble Accuracy Improvement by Feature Type')\n",
    "plt.xticks(rotation=45)\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f\"{improvements[i]:.3f}\", ha='center', va='bottom' if improvements[i] > 0 else 'top')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of accuracy distributions across folds for each model and the ensemble\n",
    "data_for_box = [individual_results[ft]['accuracy_list'] for ft in FEATURE_TYPES if ft in individual_results]\n",
    "labels = [ft.replace('_', ' ').title() for ft in FEATURE_TYPES if ft in individual_results]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.boxplot(data_for_box, labels=labels)\n",
    "plt.axhline(np.mean(ensemble_results['accuracy_list']), color='red', linestyle='--', label='Ensemble Mean')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Distribution Across Folds')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the worst and best performing fold for each model and the ensemble\n",
    "print(\"Worst and Best Fold Accuracies:\")\n",
    "for ft in FEATURE_TYPES:\n",
    "    if ft in individual_results:\n",
    "        accs = individual_results[ft]['accuracy_list']\n",
    "        print(f\"{ft.replace('_', ' ').title()}: Min={np.min(accs):.4f}, Max={np.max(accs):.4f}\")\n",
    "ensemble_accs = ensemble_results['accuracy_list']\n",
    "print(f\"Ensemble: Min={np.min(ensemble_accs):.4f}, Max={np.max(ensemble_accs):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
