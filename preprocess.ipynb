{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Preprocessing for Ensemble Learning\n",
    "\n",
    "This notebook extends the existing preprocessing to generate multiple feature types for ensemble learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "import librosa\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from modules.PostgresDBHandler import PostgresDBHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "base_dir = \"./\"\n",
    "intermediate_dir = \"ensemble_intermediate_results\"\n",
    "fixedLength = 128\n",
    "dbParams = {\n",
    "    \"dbname\": \"mydatabase\",\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"host\": \"postgres_server\",\n",
    "    \"port\": \"5432\",\n",
    "}\n",
    "\n",
    "# Feature types to generate\n",
    "FEATURE_TYPES = [\n",
    "    'mel_spectrogram', 'mfcc', 'chromagram', 'spectral_contrast',\n",
    "    'tonnetz', 'constant_q', 'cqt', 'stft', 'harmonic_percussive', 'onset_strength'\n",
    "]\n",
    "\n",
    "# Create directories for each feature type\n",
    "for feature_type in FEATURE_TYPES:\n",
    "    os.makedirs(os.path.join(intermediate_dir, feature_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database and ensure feature types exist\n",
    "db = PostgresDBHandler(**dbParams)\n",
    "db.connect()\n",
    "\n",
    "# Check if feature types exist, if not create them\n",
    "existing_feature_types = [ft['name'] for ft in db.get_all_feature_types()]\n",
    "\n",
    "feature_type_params = {\n",
    "    'mel_spectrogram': {'n_mels': 64, 'fmin': 0, 'fmax': None},\n",
    "    'mfcc': {'n_mfcc': 8, 'n_mels': 64},\n",
    "    'chromagram': {'n_chroma': 8},\n",
    "    'spectral_contrast': {'n_bands': 3},\n",
    "    'tonnetz': {},\n",
    "    'constant_q': {'bins_per_octave': 6, 'n_bins': 42},\n",
    "    'cqt': {'bins_per_octave': 6, 'n_chroma': 42},\n",
    "    'stft': {'n_fft': 512, 'hop_length': 256},\n",
    "    'harmonic_percussive': {'margin': 3.0},\n",
    "    'onset_strength': {'hop_length': 128}\n",
    "}\n",
    "\n",
    "for feature_type, params in feature_type_params.items():\n",
    "    if feature_type not in existing_feature_types:\n",
    "        description = f\"{feature_type.replace('_', ' ').title()} features\"\n",
    "        db.insert_feature_type(feature_type, description, params)\n",
    "        print(f\"Created feature type: {feature_type}\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(array, fixed_length):\n",
    "    if array.shape[1] > fixed_length:\n",
    "        return array[:, :fixed_length]\n",
    "    elif array.shape[1] < fixed_length:\n",
    "        pad_width = fixed_length - array.shape[1]\n",
    "        return np.pad(array, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "    else:\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio_data, sr):\n",
    "    \"\"\"Apply audio augmentation techniques.\"\"\"\n",
    "    # Time-stretching\n",
    "    stretched = librosa.effects.time_stretch(audio_data, rate=1.1)\n",
    "    \n",
    "    # Pitch-shifting\n",
    "    pitched = librosa.effects.pitch_shift(audio_data, sr=sr, n_steps=2)\n",
    "    \n",
    "    # Adding noise\n",
    "    noise = np.random.randn(len(audio_data))\n",
    "    audio_data_noisy = audio_data + 0.005 * noise\n",
    "    \n",
    "    # Ensure all augmented data have the same dtype\n",
    "    audio_data_noisy = audio_data_noisy.astype(np.float32)\n",
    "    stretched = stretched.astype(np.float32)\n",
    "    pitched = pitched.astype(np.float32)\n",
    "    \n",
    "    augmented_data = {\n",
    "        \"time_stretch\": stretched,\n",
    "        \"pitch_shifting\": pitched,\n",
    "        \"noise\": audio_data_noisy,\n",
    "    }\n",
    "    \n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_in_chunks(audio_path, feature_type, sr, chunk_size=10):\n",
    "    \"\"\"Extract features from an audio file in smaller chunks.\"\"\"\n",
    "    try:\n",
    "        # Open audio file\n",
    "        audio_data, _ = librosa.load(audio_path, sr=sr, offset=0, duration=None)\n",
    "        \n",
    "        # Get the length of the audio file\n",
    "        audio_length = len(audio_data)\n",
    "        \n",
    "        # Create an empty dictionary to store features\n",
    "        features = {feature_type: []}\n",
    "        \n",
    "        # Process the audio file in chunks\n",
    "        for start in range(0, audio_length, chunk_size):\n",
    "            chunk = audio_data[start:start + chunk_size]\n",
    "            if len(chunk) < chunk_size:  # Padding if chunk is smaller than desired size\n",
    "                chunk = np.pad(chunk, (0, chunk_size - len(chunk)), mode=\"constant\")\n",
    "            \n",
    "            # Adjust n_fft dynamically: make sure it's never larger than the chunk length\n",
    "            n_fft = min(2048, len(chunk))  # `n_fft` can't be larger than the chunk size\n",
    "            hop_length = n_fft // 2  # Half-overlap for hop_length\n",
    "            \n",
    "            # Check if chunk has enough signal (i.e., it's not silent)\n",
    "            non_silent_intervals = librosa.effects.split(chunk, top_db=30)  # Detect non-silent parts\n",
    "            if len(non_silent_intervals) == 0:\n",
    "                continue  # Skip this chunk if it's silent\n",
    "            \n",
    "            # Extract the feature based on feature_type\n",
    "            if feature_type == 'mel_spectrogram':\n",
    "                mel_spectrogram = librosa.feature.melspectrogram(y=chunk, sr=sr, n_mels=64)\n",
    "                features[feature_type].append(librosa.power_to_db(mel_spectrogram, ref=np.max))\n",
    "                \n",
    "            elif feature_type == 'mfcc':\n",
    "                features[feature_type].append(librosa.feature.mfcc(y=chunk, sr=sr, n_mfcc=8))\n",
    "                \n",
    "            elif feature_type == 'chromagram':\n",
    "                features[feature_type].append(librosa.feature.chroma_stft(y=chunk, sr=sr, n_chroma=8))\n",
    "                \n",
    "            elif feature_type == 'spectral_contrast':\n",
    "                features[feature_type].append(librosa.feature.spectral_contrast(y=chunk, sr=sr, n_bands=3))\n",
    "                \n",
    "            elif feature_type == 'tonnetz':\n",
    "                y_harmonic = librosa.effects.harmonic(chunk)\n",
    "                features[feature_type].append(librosa.feature.tonnetz(y=y_harmonic, sr=sr))\n",
    "                \n",
    "            elif feature_type == 'constant_q':\n",
    "                cqt = librosa.cqt(y=chunk, sr=sr, bins_per_octave=6, n_bins=42)\n",
    "                features[feature_type].append(librosa.amplitude_to_db(np.abs(cqt), ref=np.max))\n",
    "                \n",
    "            elif feature_type == 'cqt':\n",
    "                features[feature_type].append(librosa.feature.chroma_cqt(y=chunk, sr=sr, bins_per_octave=6, n_chroma=42))\n",
    "                \n",
    "            elif feature_type == 'stft':\n",
    "                stft = librosa.stft(y=chunk, n_fft=n_fft, hop_length=hop_length)\n",
    "                features[feature_type].append(librosa.amplitude_to_db(np.abs(stft), ref=np.max))\n",
    "                \n",
    "            elif feature_type == 'harmonic_percussive':\n",
    "                y_harmonic, y_percussive = librosa.effects.hpss(chunk, margin=3.0)\n",
    "                harmonic_stft = librosa.stft(y_harmonic, n_fft=n_fft, hop_length=hop_length)  # Match STFT settings\n",
    "                features[feature_type].append(librosa.amplitude_to_db(np.abs(harmonic_stft), ref=np.max))\n",
    "                \n",
    "            elif feature_type == 'onset_strength':\n",
    "                onset_env = librosa.onset.onset_strength(y=chunk, sr=sr, hop_length=hop_length)\n",
    "                features[feature_type].append(onset_env.reshape(1, -1))\n",
    "            \n",
    "        # Return the extracted features (concatenating across chunks)\n",
    "        return np.concatenate(features[feature_type], axis=-1)  # Concatenate features across chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features from {audio_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_ensemble(audio_index, db_params):\n",
    "    \"\"\"Process a single audio file and generate features incrementally.\"\"\"\n",
    "    db = PostgresDBHandler(**db_params)\n",
    "    db.connect()\n",
    "    \n",
    "    try:\n",
    "        db_data = db.get_audio_file(audio_index)\n",
    "        audio_path = db_data['filePath']\n",
    "        sr = db_data[\"sampleRate\"]\n",
    "        \n",
    "        # Get feature type IDs\n",
    "        feature_type_ids = {}\n",
    "        for feature_type in FEATURE_TYPES:\n",
    "            feature_type_ids[feature_type] = db.get_feature_type_id(feature_type)\n",
    "        \n",
    "        # Process each feature type incrementally\n",
    "        for feature_type in FEATURE_TYPES:\n",
    "            print(f\"Processing feature: {feature_type}\")\n",
    "            feature_data = extract_feature_in_chunks(audio_path, feature_type, sr)\n",
    "            \n",
    "            if feature_data is not None:\n",
    "                unique_id = uuid.uuid4()\n",
    "                feature_path = os.path.join(intermediate_dir, feature_type, f\"{unique_id}_{feature_type}.npy\")\n",
    "                \n",
    "                np.save(feature_path, feature_data)\n",
    "                \n",
    "                # Insert processed feature into the database\n",
    "                db.insert_processed_audio(\n",
    "                    db_data[\"instrumentID\"],\n",
    "                    db_data[\"audioID\"],\n",
    "                    fixedLength,\n",
    "                    feature_type_ids[feature_type],\n",
    "                    feature_path,\n",
    "                    \"original\",\n",
    "                )\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio {audio_index}: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ensemble_preprocessing(audios_ids, db_params, n_jobs=-1):\n",
    "    \"\"\"Apply preprocessing with all feature types using parallel processing.\"\"\"\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_file_ensemble)(audio_index, db_params) for audio_index in audios_ids\n",
    "    )\n",
    "    \n",
    "    successful = sum(results)\n",
    "    total = len(audios_ids)\n",
    "    print(f\"Successfully processed {successful}/{total} audio files\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all audio IDs and process them\n",
    "db = PostgresDBHandler(**dbParams)\n",
    "db.connect()\n",
    "audio_ids = db.get_all_audio_ids()\n",
    "print(f\"Found {len(audio_ids)} audio files to process\")\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_ensemble_preprocessing(audio_ids, dbParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function to compare different feature types\n",
    "def visualize_features_comparison(audio_file_path, sr=22050):\n",
    "    \"\"\"Visualize all feature types for a single audio file.\"\"\"\n",
    "    audio, _ = librosa.load(audio_file_path, sr=sr)\n",
    "    features = extract_feature_in_chunks(audio, sr)\n",
    "    \n",
    "    # Create subplots\n",
    "    n_features = len(features)\n",
    "    cols = 3\n",
    "    rows = (n_features + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    axes = axes.flatten() if rows > 1 else [axes] if cols == 1 else axes\n",
    "    \n",
    "    for i, (feature_name, feature_data) in enumerate(features.items()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if feature_name in ['mel_spectrogram', 'constant_q', 'stft', 'harmonic_percussive']:\n",
    "            librosa.display.specshow(feature_data, sr=sr, x_axis='time', y_axis='mel', ax=ax)\n",
    "            ax.set_title(f'{feature_name.replace(\"_\", \" \").title()}')\n",
    "        elif feature_name == 'mfcc':\n",
    "            librosa.display.specshow(feature_data, x_axis='time', ax=ax)\n",
    "            ax.set_title('MFCC')\n",
    "        elif feature_name in ['chromagram', 'cqt']:\n",
    "            librosa.display.specshow(feature_data, x_axis='time', y_axis='chroma', ax=ax)\n",
    "            ax.set_title(f'{feature_name.replace(\"_\", \" \").title()}')\n",
    "        elif feature_name == 'spectral_contrast':\n",
    "            librosa.display.specshow(feature_data, x_axis='time', ax=ax)\n",
    "            ax.set_title('Spectral Contrast')\n",
    "        elif feature_name == 'tonnetz':\n",
    "            librosa.display.specshow(feature_data, x_axis='time', ax=ax)\n",
    "            ax.set_title('Tonal Centroids')\n",
    "        elif feature_name == 'onset_strength':\n",
    "            ax.plot(feature_data[0])\n",
    "            ax.set_title('Onset Strength')\n",
    "            ax.set_ylabel('Strength')\n",
    "            ax.set_xlabel('Time')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics about processed data\n",
    "def print_processing_stats():\n",
    "    \"\"\"Print statistics about the processed data.\"\"\"\n",
    "    db = PostgresDBHandler(**dbParams)\n",
    "    db.connect()\n",
    "    \n",
    "    print(\"=== Processing Statistics ===\")\n",
    "    \n",
    "    # Get counts for each feature type\n",
    "    for feature_type in FEATURE_TYPES:\n",
    "        processed_data = db.get_processed_data_by_feature_type(feature_type)\n",
    "        print(f\"{feature_type}: {len(processed_data)} samples\")\n",
    "    \n",
    "    # Get augmentation statistics\n",
    "    query = \"\"\"\n",
    "    SELECT augmentation, COUNT(*) as count \n",
    "    FROM Processed \n",
    "    GROUP BY augmentation\n",
    "    \"\"\"\n",
    "    db.execute_query(query)\n",
    "    augmentation_stats = db.fetchall()\n",
    "    \n",
    "    print(\"\\n=== Augmentation Statistics ===\")\n",
    "    for aug, count in augmentation_stats:\n",
    "        print(f\"{aug}: {count} samples\")\n",
    "    \n",
    "    db.close()\n",
    "\n",
    "print_processing_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
