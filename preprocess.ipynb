{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Preprocessing for Ensemble Learning\n",
    "\n",
    "This notebook extends the existing preprocessing to generate multiple feature types for ensemble learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "import librosa\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from modules.PostgresDBHandler import PostgresDBHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "base_dir = \"./\"\n",
    "intermediate_dir = \"ensemble_intermediate_results\"\n",
    "fixedLength = 128\n",
    "dbParams = {\n",
    "    \"dbname\": \"mydatabase\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"password\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "}\n",
    "\n",
    "# Feature types to generate\n",
    "FEATURE_TYPES = [\n",
    "    'mel_spectrogram', 'mfcc', 'chromagram', 'spectral_contrast',\n",
    "    'tonnetz', 'constant_q', 'cqt', 'stft', 'harmonic_percussive', 'onset_strength'\n",
    "]\n",
    "\n",
    "# Create directories for each feature type\n",
    "for feature_type in FEATURE_TYPES:\n",
    "    os.makedirs(os.path.join(intermediate_dir, feature_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database and ensure feature types exist\n",
    "db = PostgresDBHandler(**dbParams)\n",
    "db.connect()\n",
    "\n",
    "# Check if feature types exist, if not create them\n",
    "existing_feature_types = [ft['name'] for ft in db.get_all_feature_types()]\n",
    "\n",
    "feature_type_params = {\n",
    "    'mel_spectrogram': {'n_mels': 128, 'fmin': 0, 'fmax': None},\n",
    "    'mfcc': {'n_mfcc': 13, 'n_mels': 128},\n",
    "    'chromagram': {'n_chroma': 12},\n",
    "    'spectral_contrast': {'n_bands': 6},\n",
    "    'tonnetz': {},\n",
    "    'constant_q': {'bins_per_octave': 12, 'n_bins': 84},\n",
    "    'cqt': {'bins_per_octave': 12, 'n_bins': 84},\n",
    "    'stft': {'n_fft': 2048, 'hop_length': 512},\n",
    "    'harmonic_percussive': {'margin': 3.0},\n",
    "    'onset_strength': {'hop_length': 512}\n",
    "}\n",
    "\n",
    "for feature_type, params in feature_type_params.items():\n",
    "    if feature_type not in existing_feature_types:\n",
    "        description = f\"{feature_type.replace('_', ' ').title()} features\"\n",
    "        db.insert_feature_type(feature_type, description, params)\n",
    "        print(f\"Created feature type: {feature_type}\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(array, fixed_length):\n",
    "    if array.shape[1] > fixed_length:\n",
    "        return array[:, :fixed_length]\n",
    "    elif array.shape[1] < fixed_length:\n",
    "        pad_width = fixed_length - array.shape[1]\n",
    "        return np.pad(array, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "    else:\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio_data, sr):\n",
    "    \"\"\"Apply audio augmentation techniques.\"\"\"\n",
    "    # Time-stretching\n",
    "    stretched = librosa.effects.time_stretch(audio_data, rate=1.1)\n",
    "    \n",
    "    # Pitch-shifting\n",
    "    pitched = librosa.effects.pitch_shift(audio_data, sr=sr, n_steps=2)\n",
    "    \n",
    "    # Adding noise\n",
    "    noise = np.random.randn(len(audio_data))\n",
    "    audio_data_noisy = audio_data + 0.005 * noise\n",
    "    \n",
    "    # Ensure all augmented data have the same dtype\n",
    "    audio_data_noisy = audio_data_noisy.astype(np.float32)\n",
    "    stretched = stretched.astype(np.float32)\n",
    "    pitched = pitched.astype(np.float32)\n",
    "    \n",
    "    augmented_data = {\n",
    "        \"time_stretch\": stretched,\n",
    "        \"pitch_shifting\": pitched,\n",
    "        \"noise\": audio_data_noisy,\n",
    "    }\n",
    "    \n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(audio_data, sr):\n",
    "    \"\"\"Extract all feature types for ensemble learning.\"\"\"\n",
    "    if audio_data.ndim > 1:\n",
    "        audio_data = librosa.to_mono(audio_data)\n",
    "    \n",
    "    audio_data = librosa.util.normalize(audio_data)\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Mel spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_mels=128)\n",
    "        features['mel_spectrogram'] = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        \n",
    "        # MFCC\n",
    "        features['mfcc'] = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n",
    "        \n",
    "        # Chromagram\n",
    "        features['chromagram'] = librosa.feature.chroma_stft(y=audio_data, sr=sr, n_chroma=12)\n",
    "        \n",
    "        # Spectral contrast\n",
    "        features['spectral_contrast'] = librosa.feature.spectral_contrast(y=audio_data, sr=sr, n_bands=6)\n",
    "        \n",
    "        # Tonal centroids\n",
    "        y_harmonic = librosa.effects.harmonic(audio_data)\n",
    "        features['tonnetz'] = librosa.feature.tonnetz(y=y_harmonic, sr=sr)\n",
    "        \n",
    "        # Constant-Q transform\n",
    "        cqt = librosa.cqt(y=audio_data, sr=sr, bins_per_octave=12, n_bins=84)\n",
    "        features['constant_q'] = librosa.amplitude_to_db(np.abs(cqt), ref=np.max)\n",
    "        \n",
    "        # CQT chromagram\n",
    "        features['cqt'] = librosa.feature.chroma_cqt(y=audio_data, sr=sr, bins_per_octave=12, n_bins=84)\n",
    "        \n",
    "        # STFT\n",
    "        stft = librosa.stft(y=audio_data, n_fft=2048, hop_length=512)\n",
    "        features['stft'] = librosa.amplitude_to_db(np.abs(stft), ref=np.max)\n",
    "        \n",
    "        # Harmonic-percussive\n",
    "        y_harmonic, y_percussive = librosa.effects.hpss(audio_data, margin=3.0)\n",
    "        harmonic_stft = librosa.stft(y_harmonic)\n",
    "        features['harmonic_percussive'] = librosa.amplitude_to_db(np.abs(harmonic_stft), ref=np.max)\n",
    "        \n",
    "        # Onset strength\n",
    "        onset_env = librosa.onset.onset_strength(y=audio_data, sr=sr, hop_length=512)\n",
    "        features['onset_strength'] = onset_env.reshape(1, -1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting features: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_ensemble(audio_index, db_params):\n",
    "    \"\"\"Process a single audio file and generate all feature types.\"\"\"\n",
    "    db = PostgresDBHandler(**db_params)\n",
    "    db.connect()\n",
    "    \n",
    "    try:\n",
    "        db_data = db.get_audio_file(audio_index)\n",
    "        audio, _ = librosa.load(db_data['filePath'], sr=db_data[\"sampleRate\"])\n",
    "        \n",
    "        # Get feature type IDs\n",
    "        feature_type_ids = {}\n",
    "        for feature_type in FEATURE_TYPES:\n",
    "            feature_type_ids[feature_type] = db.get_feature_type_id(feature_type)\n",
    "        \n",
    "        # Process original audio\n",
    "        features = extract_all_features(audio, db_data[\"sampleRate\"])\n",
    "        \n",
    "        # Save features and insert into database\n",
    "        for feature_type, feature_data in features.items():\n",
    "            unique_id = uuid.uuid4()\n",
    "            feature_path = os.path.join(intermediate_dir, feature_type, f\"{unique_id}_{feature_type}.npy\")\n",
    "            \n",
    "            np.save(feature_path, pad_or_truncate(feature_data, fixedLength))\n",
    "            \n",
    "            db.insert_processed_audio(\n",
    "                db_data[\"instrumentID\"],\n",
    "                db_data[\"audioID\"],\n",
    "                fixedLength,\n",
    "                feature_type_ids[feature_type],\n",
    "                feature_path,\n",
    "                \"original\",\n",
    "            )\n",
    "        \n",
    "        # Apply data augmentation\n",
    "        augmented_audios = augment_audio(audio, db_data[\"sampleRate\"])\n",
    "        \n",
    "        for aug_key, aug_audio in augmented_audios.items():\n",
    "            aug_features = extract_all_features(aug_audio, db_data[\"sampleRate\"])\n",
    "            \n",
    "            for feature_type, feature_data in aug_features.items():\n",
    "                unique_id = uuid.uuid4()\n",
    "                feature_path = os.path.join(intermediate_dir, feature_type, f\"{unique_id}_{feature_type}.npy\")\n",
    "                \n",
    "                np.save(feature_path, pad_or_truncate(feature_data, fixedLength))\n",
    "                \n",
    "                db.insert_processed_audio(\n",
    "                    db_data[\"instrumentID\"],\n",
    "                    db_data[\"audioID\"],\n",
    "                    fixedLength,\n",
    "                    feature_type_ids[feature_type],\n",
    "                    feature_path,\n",
    "                    aug_key,\n",
    "                )\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio {audio_index}: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ensemble_preprocessing(audios_ids, db_params, n_jobs=-1):\n",
    "    \"\"\"Apply preprocessing with all feature types using parallel processing.\"\"\"\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_file_ensemble)(audio_index, db_params) for audio_index in audios_ids\n",
    "    )\n",
    "    \n",
    "    successful = sum(results)\n",
    "    total = len(audios_ids)\n",
    "    print(f\"Successfully processed {successful}/{total} audio files\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all audio IDs and process them\n",
    "db = PostgresDBHandler(**dbParams)\n",
    "db.connect()\n",
    "audio_ids = db.get_all_audio_ids()\n",
    "print(f\"Found {len(audio_ids)} audio files to process\")\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_ensemble_preprocessing(audio_ids, dbParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function to compare different feature types\n",
    "def visualize_features_comparison(audio_file_path, sr=22050):\n",
    "    \"\"\"Visualize all feature types for a single audio file.\"\"\"\n",
    "    audio, _ = librosa.load(audio_file_path, sr=sr)\n",
    "    features = extract_all_features(audio, sr)\n",
    "    \n",
    "    # Create subplots\n",
    "    n_features = len(features)\n",
    "    cols = 3\n",
    "    rows = (n_features + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    axes = axes.flatten() if rows > 1 else [axes] if cols == 1 else axes\n",
    "    \n",
    "    for i, (feature_name, feature_data) in enumerate(features.items()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if feature_name in ['mel_spectrogram', 'constant_q', 'stft', 'harmonic_percussive']:\n",
    "            librosa.display.specshow(feature_data, sr=sr, x_axis='time', y_axis='mel', ax=ax)\n",
    "            ax.set_title(f'{feature_name.replace(\"_\", \" \").title()}')\n",
    "        elif feature_name == 'mfcc':\n",
    "            librosa.display.specshow(feature_data, x_axis='time', ax=ax)\n",
    "            ax.set_title('MFCC')\n",
    "        elif feature_name in ['chromagram', 'cqt']:\n",
    "            librosa.display.specshow(feature_data, x_axis='time', y_axis='chroma', ax=ax)\n",
    "            ax.set_title(f'{feature_name.replace(\"_\", \" \").title()}')\n",
    "        elif feature_name == 'spectral_contrast':\n",
    "            librosa.display.specshow(feature_data, x_axis='time', ax=ax)\n",
    "            ax.set_title('Spectral Contrast')\n",
    "        elif feature_name == 'tonnetz':\n",
    "            librosa.display.specshow(feature_data, x_axis='time', ax=ax)\n",
    "            ax.set_title('Tonal Centroids')\n",
    "        elif feature_name == 'onset_strength':\n",
    "            ax.plot(feature_data[0])\n",
    "            ax.set_title('Onset Strength')\n",
    "            ax.set_ylabel('Strength')\n",
    "            ax.set_xlabel('Time')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics about processed data\n",
    "def print_processing_stats():\n",
    "    \"\"\"Print statistics about the processed data.\"\"\"\n",
    "    db = PostgresDBHandler(**dbParams)\n",
    "    db.connect()\n",
    "    \n",
    "    print(\"=== Processing Statistics ===\")\n",
    "    \n",
    "    # Get counts for each feature type\n",
    "    for feature_type in FEATURE_TYPES:\n",
    "        processed_data = db.get_processed_data_by_feature_type(feature_type)\n",
    "        print(f\"{feature_type}: {len(processed_data)} samples\")\n",
    "    \n",
    "    # Get augmentation statistics\n",
    "    query = \"\"\"\n",
    "    SELECT augmentation, COUNT(*) as count \n",
    "    FROM Processed \n",
    "    GROUP BY augmentation\n",
    "    \"\"\"\n",
    "    db.execute_query(query)\n",
    "    augmentation_stats = db.fetchall()\n",
    "    \n",
    "    print(\"\\n=== Augmentation Statistics ===\")\n",
    "    for aug, count in augmentation_stats:\n",
    "        print(f\"{aug}: {count} samples\")\n",
    "    \n",
    "    db.close()\n",
    "\n",
    "print_processing_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
