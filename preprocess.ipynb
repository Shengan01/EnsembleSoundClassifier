{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Preprocessing for Ensemble Learning\n",
    "\n",
    "This notebook extends the existing preprocessing to generate multiple feature types for ensemble learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import librosa\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from modules.PostgresDBHandler import PostgresDBHandler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "base_dir = \"./\"\n",
    "intermediate_dir = \"ensemble_intermediate_results\"\n",
    "fixedLength = 128\n",
    "dbParams = {\n",
    "    \"dbname\": \"mydatabase\",\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"host\": \"postgres_server\",\n",
    "    \"port\": \"5432\",\n",
    "}\n",
    "\n",
    "# Feature types to generate\n",
    "FEATURE_TYPES = [\n",
    "    'mel_spectrogram', 'mfcc', 'chromagram', 'spectral_contrast',\n",
    "    'tonnetz', 'constant_q', 'cqt', 'stft', 'harmonic_percussive', 'onset_strength'\n",
    "]\n",
    "\n",
    "# Create directories for each feature type\n",
    "for feature_type in FEATURE_TYPES:\n",
    "    os.makedirs(os.path.join(intermediate_dir, feature_type), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database and ensure feature types exist\n",
    "db = PostgresDBHandler(**dbParams)\n",
    "db.connect()\n",
    "\n",
    "# Check if feature types exist, if not create them\n",
    "existing_feature_types = [ft['name'] for ft in db.get_all_feature_types()]\n",
    "\n",
    "FEATURE_SHAPES = {\n",
    "    'mel_spectrogram': (64, 128),\n",
    "    'mfcc': (8, 128),\n",
    "    'chromagram': (8, 128),\n",
    "    'spectral_contrast': (3, 128),\n",
    "    'tonnetz': (6, 128),\n",
    "    'constant_q': (42, 128),\n",
    "    'cqt': (42, 128),\n",
    "    'stft': (512, 128),\n",
    "    'harmonic_percussive': (1025, 128),\n",
    "    'onset_strength': (1, 128)\n",
    "}\n",
    "\n",
    "feature_type_params = {\n",
    "    'mel_spectrogram': {'n_mels': 64, 'fmin': 0, 'fmax': None},\n",
    "    'mfcc': {'n_mfcc': 8, 'n_mels': 64},\n",
    "    'chromagram': {'n_chroma': 8},\n",
    "    'spectral_contrast': {'n_bands': 2},  # To get 3 bands in output (n_bands+1)\n",
    "    'tonnetz': {},\n",
    "    'constant_q': {'bins_per_octave': 6, 'n_bins': 42},  # 7 octaves x 6 bins/octave\n",
    "    'cqt': {'bins_per_octave': 12, 'n_bins': 84},  # 7 octaves x 12 bins/octave (standard musical scale)\n",
    "    'stft': {'n_fft': 512, 'hop_length': 256},\n",
    "    'harmonic_percussive': {'margin': 3.0, 'n_fft': 2048, 'hop_length': 1024},\n",
    "    'onset_strength': {'hop_length': 128}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for feature_type, params in feature_type_params.items():\n",
    "    if feature_type not in existing_feature_types:\n",
    "        description = f\"{feature_type.replace('_', ' ').title()} features\"\n",
    "        db.insert_feature_type(feature_type, description, params)\n",
    "        print(f\"Created feature type: {feature_type}\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(array, fixed_length):\n",
    "    # If fixed_length is a tuple (e.g., (freq_bins, time_steps)), get the time_steps\n",
    "    if isinstance(fixed_length, tuple):\n",
    "        fixed_length = fixed_length[1]\n",
    "\n",
    "    # Defensive check: if array has fewer than 2 dims, raise error\n",
    "    if array.ndim < 2:\n",
    "        raise ValueError(\"Input array must have at least 2 dimensions\")\n",
    "\n",
    "    current_length = array.shape[1]\n",
    "\n",
    "    if current_length > fixed_length:\n",
    "        return array[:, :fixed_length]\n",
    "    elif current_length < fixed_length:\n",
    "        pad_width = fixed_length - current_length\n",
    "        return np.pad(array, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
    "    else:\n",
    "        return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio_data, sr):\n",
    "    \"\"\"Apply audio augmentation techniques.\"\"\"\n",
    "    # Time-stretching\n",
    "    stretched = librosa.effects.time_stretch(audio_data, rate=1.1)\n",
    "    \n",
    "    # Pitch-shifting\n",
    "    pitched = librosa.effects.pitch_shift(audio_data, sr=sr, n_steps=2)\n",
    "    \n",
    "    # Adding noise\n",
    "    noise = np.random.randn(len(audio_data))\n",
    "    audio_data_noisy = audio_data + 0.005 * noise\n",
    "    \n",
    "    # Ensure all augmented data have the same dtype\n",
    "    audio_data_noisy = audio_data_noisy.astype(np.float32)\n",
    "    stretched = stretched.astype(np.float32)\n",
    "    pitched = pitched.astype(np.float32)\n",
    "    \n",
    "    augmented_data = {\n",
    "        \"time_stretch\": stretched,\n",
    "        \"pitch_shifting\": pitched,\n",
    "        \"noise\": audio_data_noisy,\n",
    "    }\n",
    "    \n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_in_chunks(audio_data, sr, chunk_size=5.0, hop_size=2.5):\n",
    "    feature_outputs = defaultdict(list)\n",
    "    total_length = len(audio_data)\n",
    "    chunk_sample_length = int(chunk_size * sr)\n",
    "    hop_sample_length = int(hop_size * sr)\n",
    "\n",
    "    def process_chunk(start):\n",
    "        end = start + chunk_sample_length\n",
    "        if end > total_length:\n",
    "            chunk = np.pad(audio_data[start:], (0, end - total_length))\n",
    "        else:\n",
    "            chunk = audio_data[start:end]\n",
    "\n",
    "        if len(chunk) < 64 or np.max(np.abs(chunk)) < 1e-5:\n",
    "            return None\n",
    "\n",
    "        chunk_features = {}\n",
    "\n",
    "        for feature_type, params in feature_type_params.items():\n",
    "            try:\n",
    "                local_params = dict(params)\n",
    "                if 'n_fft' in local_params:\n",
    "                    local_params['n_fft'] = min(local_params['n_fft'], len(chunk))\n",
    "\n",
    "                if feature_type == 'mel_spectrogram':\n",
    "                    S = librosa.feature.melspectrogram(y=chunk, sr=sr, **local_params)\n",
    "                    feat = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "                elif feature_type == 'mfcc':\n",
    "                    feat = librosa.feature.mfcc(y=chunk, sr=sr, **local_params)\n",
    "\n",
    "                elif feature_type == 'chromagram':\n",
    "                    feat = librosa.feature.chroma_stft(y=chunk, sr=sr, **local_params)\n",
    "\n",
    "                elif feature_type == 'spectral_contrast':\n",
    "                    feat = librosa.feature.spectral_contrast(y=chunk, sr=sr, **local_params)\n",
    "\n",
    "                elif feature_type == 'tonnetz':\n",
    "                    y_harmonic = librosa.effects.harmonic(chunk)\n",
    "                    feat = librosa.feature.tonnetz(y=y_harmonic, sr=sr)\n",
    "\n",
    "                elif feature_type == 'constant_q':\n",
    "                    C = np.abs(librosa.cqt(y=chunk, sr=sr, **local_params))\n",
    "                    feat = librosa.amplitude_to_db(C, ref=np.max)\n",
    "\n",
    "                elif feature_type == 'cqt':\n",
    "                    C = np.abs(librosa.cqt(y=chunk, sr=sr, **local_params))\n",
    "                    feat = librosa.amplitude_to_db(C, ref=np.max)\n",
    "\n",
    "                elif feature_type == 'stft':\n",
    "                    S = np.abs(librosa.stft(y=chunk, **local_params))\n",
    "                    feat = librosa.amplitude_to_db(S, ref=np.max)\n",
    "\n",
    "                elif feature_type == 'harmonic_percussive':\n",
    "                    D = librosa.stft(y=chunk, n_fft=local_params.get('n_fft', 2048), hop_length=local_params.get('hop_length', 1024))\n",
    "                    H, P = librosa.decompose.hpss(D)\n",
    "                    H_db = librosa.amplitude_to_db(np.abs(H), ref=np.max)\n",
    "                    P_db = librosa.amplitude_to_db(np.abs(P), ref=np.max)\n",
    "                    feat = np.vstack([H_db, P_db])\n",
    "\n",
    "                elif feature_type == 'onset_strength':\n",
    "                    onset_env = librosa.onset.onset_strength(y=chunk, sr=sr, **local_params)\n",
    "                    feat = onset_env[np.newaxis, :]\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                chunk_features[feature_type] = feat\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Feature extraction failed for {feature_type} on chunk starting at {start}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return chunk_features\n",
    "\n",
    "    # Parallel chunk processing\n",
    "    chunk_results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_chunk)(start)\n",
    "        for start in range(0, total_length, hop_sample_length)\n",
    "    )\n",
    "\n",
    "    for chunk_result in chunk_results:\n",
    "        if chunk_result is None:\n",
    "            continue\n",
    "        for feature_type, feat in chunk_result.items():\n",
    "            feature_outputs[feature_type].append(feat)\n",
    "\n",
    "    processed_features = {}\n",
    "    for feature_name, chunks in feature_outputs.items():\n",
    "        if not chunks:\n",
    "            continue\n",
    "        feat = np.concatenate(chunks, axis=1)\n",
    "        target_shape = FEATURE_SHAPES[feature_name]\n",
    "        feat = pad_or_truncate(feat, target_shape)\n",
    "        processed_features[feature_name] = feat\n",
    "\n",
    "    return processed_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_ensemble(audio_index, db_params):\n",
    "    import traceback\n",
    "    db = PostgresDBHandler(**db_params)\n",
    "    db.connect()\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    try:\n",
    "        db_data = db.get_audio_file(audio_index)\n",
    "        audio_path = db_data['filePath']\n",
    "        sr = db_data[\"sampleRate\"]\n",
    "\n",
    "        if not os.path.exists(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "        y, _ = librosa.load(audio_path, sr=None)\n",
    "\n",
    "        if y is None or len(y) == 0:\n",
    "            raise ValueError(f\"Empty audio data: {audio_path}\")\n",
    "\n",
    "        feature_type_ids = {ftype: db.get_feature_type_id(ftype) for ftype in FEATURE_TYPES}\n",
    "        features = extract_feature_in_chunks(y, sr)\n",
    "\n",
    "        for feature_type, data in features.items():\n",
    "            unique_id = uuid.uuid4()\n",
    "            feature_path = os.path.join(intermediate_dir, feature_type, f\"{unique_id}_{feature_type}.npy\")\n",
    "            os.makedirs(os.path.dirname(feature_path), exist_ok=True)\n",
    "            np.save(feature_path, data)\n",
    "\n",
    "            db.insert_processed_audio(\n",
    "                db_data[\"instrumentID\"],\n",
    "                db_data[\"audioID\"],\n",
    "                fixedLength,\n",
    "                feature_type_ids[feature_type],\n",
    "                feature_path,\n",
    "                \"original\",\n",
    "            )\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio {audio_index}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    finally:\n",
    "        db.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ensemble_preprocessing(audios_ids, db_params, n_jobs=-1):\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_file_ensemble)(audio_index, db_params) for audio_index in tqdm(audios_ids)\n",
    "    )\n",
    "    successful = sum(results)\n",
    "    total = len(audios_ids)\n",
    "    print(f\"Successfully processed {successful}/{total} audio files\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all audio IDs and process them\n",
    "db = PostgresDBHandler(**dbParams)\n",
    "db.connect()\n",
    "audio_ids = db.get_all_audio_ids()\n",
    "print(f\"Found {len(audio_ids)} audio files to process\")\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_ensemble_preprocessing(audio_ids, dbParams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features_comparison(audio_file_path, label=None, sr=22050):\n",
    "    y, loaded_sr = librosa.load(audio_file_path, sr=sr)\n",
    "    features = extract_feature_in_chunks(y, loaded_sr)  # Extract all features at once\n",
    "\n",
    "    n_features = len(features)\n",
    "    cols = 3\n",
    "    rows = (n_features + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    axes = axes.flatten() if rows > 1 else [axes] if cols == 1 else axes\n",
    "\n",
    "    if label is not None:\n",
    "        fig.suptitle(f\"Sample Label: {label}\", fontsize=16)\n",
    "\n",
    "    for i, (feature_name, feature_data) in enumerate(features.items()):\n",
    "        ax = axes[i]\n",
    "\n",
    "        if feature_name in ['mel_spectrogram', 'constant_q', 'stft', 'harmonic_percussive']:\n",
    "            librosa.display.specshow(feature_data, sr=loaded_sr, x_axis='time', y_axis='mel', ax=ax)\n",
    "            ax.set_title(f'{feature_name.replace(\"_\", \" \").title()}')\n",
    "        elif feature_name == 'mfcc':\n",
    "            librosa.display.specshow(feature_data, x_axis='time', ax=ax)\n",
    "            ax.set_title('MFCC')\n",
    "        elif feature_name in ['chromagram', 'cqt']:\n",
    "            librosa.display.specshow(feature_data, x_axis='time', y_axis='chroma', ax=ax)\n",
    "            ax.set_title(f'{feature_name.replace(\"_\", \" \").title()}')\n",
    "        elif feature_name == 'spectral_contrast':\n",
    "            librosa.display.specshow(feature_data, x_axis='time', ax=ax)\n",
    "            ax.set_title('Spectral Contrast')\n",
    "        elif feature_name == 'tonnetz':\n",
    "            librosa.display.specshow(feature_data, x_axis='time', ax=ax)\n",
    "            ax.set_title('Tonal Centroids')\n",
    "        elif feature_name == 'onset_strength':\n",
    "            ax.plot(feature_data[0])\n",
    "            ax.set_title('Onset Strength')\n",
    "            ax.set_ylabel('Strength')\n",
    "            ax.set_xlabel('Time')\n",
    "\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    if label:\n",
    "        plt.suptitle(f\"Instrument: {label}\", fontsize=16, y=1.02)    \n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # leave space for suptitle\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = PostgresDBHandler(**dbParams)\n",
    "db.connect()\n",
    "\n",
    "# Get random audio ID and data\n",
    "audio_ids = db.get_all_audio_ids()\n",
    "random_audio_id = random.choice(audio_ids)\n",
    "audio_data = db.get_audio_file(random_audio_id)\n",
    "audio_path = audio_data[\"filePath\"]\n",
    "\n",
    "# Extract instrument label using existing execute_query + fetchone\n",
    "instrument_id = audio_data[\"instrumentID\"]\n",
    "db.execute_query(\"SELECT name FROM Instruments WHERE instrumentID = %s\", (instrument_id,))\n",
    "instrument_name = db.fetchone()[0]\n",
    "\n",
    "db.close()\n",
    "\n",
    "# Pass label to visualization\n",
    "visualize_features_comparison(audio_path, sr=22050, label=instrument_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics about processed data\n",
    "def print_processing_stats():\n",
    "    \"\"\"Print statistics about the processed data.\"\"\"\n",
    "    db = PostgresDBHandler(**dbParams)\n",
    "    db.connect()\n",
    "    \n",
    "    print(\"=== Processing Statistics ===\")\n",
    "    \n",
    "    # Get counts for each feature type\n",
    "    for feature_type in FEATURE_TYPES:\n",
    "        processed_data = db.get_processed_data_by_feature_type(feature_type)\n",
    "        print(f\"{feature_type}: {len(processed_data)} samples\")\n",
    "    \n",
    "    # Get augmentation statistics\n",
    "    query = \"\"\"\n",
    "    SELECT augmentation, COUNT(*) as count \n",
    "    FROM Processed \n",
    "    GROUP BY augmentation\n",
    "    \"\"\"\n",
    "    db.execute_query(query)\n",
    "    augmentation_stats = db.fetchall()\n",
    "    \n",
    "    print(\"\\n=== Augmentation Statistics ===\")\n",
    "    for aug, count in augmentation_stats:\n",
    "        print(f\"{aug}: {count} samples\")\n",
    "    \n",
    "    db.close()\n",
    "\n",
    "print_processing_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
